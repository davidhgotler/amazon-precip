{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Predicting Rainfall Anomalies in the Amazon\n","## Model Outline:\n","- Preprocessing\n","- Multivariate EOF\n","- Ridge Regression\n","- Lasso Regression\n","- Neural Network Regression\n","\n","Created by David Gotler, 2021"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing\n","### Define a dataframe class to:\n","- Load in the datasets:\n","    - feature dataset (ERA5): variables are CAPE, CIN, geopotential, relative humidity\n","        - lat: (0,-30), lon: (-40,-80)\n","    - target dataset (GPCC): variable is precipitation\n","        - lat: (-5,-25), lon: (-50,-70)\n","- Detrend the dataset by removing linear trends. We want the dataset to be stationary\n","- Calculate standardized anomalies by subtracting the monthly mean and dividing by standard deviation. We want centered dataset with unit variance."]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Users/davidgotler/opt/miniconda3/lib/python3.9/site-packages/xarray/core/nputils.py:166: RankWarning: Polyfit may be poorly conditioned\n","  warn_on_deficient_rank(rank, x.shape[1])\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n","/Volumes/Storage HD/Documents/RongResearch/amazon-precip/dataframe.py:118: RuntimeWarning: invalid value encountered in true_divide\n","  get_std_anom = lambda x, m, s: (x - m) / s\n"]}],"source":["from matplotlib import pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error\n","    \n","from dataframe import dataframe\n","\n","# directories\n","data_dir = 'data_store/'\n","features_fname = 'ERA5.features.1979-2021.nc'\n","target_fname = 'precip.mon.total.1x1.v2020.nc'\n","\n","# Create df object to store our feature and target datasets\n","df = dataframe(data_dir,features_fname,target_fname)\n","df.detrend()\n","df.std_anom()\n","df.remove_nan()\n","df.flatten()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<xarray.DataArray 'sample' (sample: 37)>\n","array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n","       36])\n","Dimensions without coordinates: sample\n"]},{"ename":"ValueError","evalue":"Could not convert object to NumPy datetime","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/var/folders/qg/vqplq9l13qs4nr7_9qqc98_m0000gn/T/ipykernel_35733/1912208379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mforecaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDirectForecaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_months\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minclude_autoreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_da\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_da\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mforecaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean absolute error on the test set: {:.3f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Volumes/Storage HD/Documents/RongResearch/amazon-precip/forecast.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_months\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mX_m\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_training_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mforecaster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_m\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Volumes/Storage HD/Documents/RongResearch/amazon-precip/forecast.py\u001b[0m in \u001b[0;36mget_training_matrix\u001b[0;34m(self, m, s, X, y)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mX_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear2date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_m\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time.year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_m\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time.month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mX_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlag_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_m\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeg2rad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Volumes/Storage HD/Documents/RongResearch/amazon-precip/forecast.py\u001b[0m in \u001b[0;36myear2date\u001b[0;34m(self, yrs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0myrs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myrs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     def sel_months(\n","\u001b[0;32m/Volumes/Storage HD/Documents/RongResearch/amazon-precip/forecast.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0myrs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myrs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     def sel_months(\n","\u001b[0;31mValueError\u001b[0m: Could not convert object to NumPy datetime"]}],"source":["from forecast import DirectForecaster,RecursiveForecaster\n","forecaster = DirectForecaster(LinearRegression(),lags=3,target_months=[9,10,11],steps=3,include_autoreg=True,avg_features=False)\n","X_train,y_train,X_test,y_test = forecaster.train_test_split([2016,2017,2018,2019],df.target_da,df.features_da)\n","forecaster.fit(X_train,y_train)\n","y_pred = forecaster.predict(X_test,y_test)\n","print('mean absolute error on the test set: {:.3f}%'.format(mean_absolute_error(y_test,y_pred)*100))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = forecast(df)\n","model.df.features_ds = model.avg_features()\n","features_pcs,features_eof,features_var = model.eof_features()\n","data_splits = model.forecast_split(features_pcs,model.df.target_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error\n","mean_absolute_error(y_true,y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_months = np.array([9,10,11])\n","lags = 3\n","steps = 3\n","feature_months = np.arange(target_months.min()-lags,target_months.max()-steps+1)\n","feature_months"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","reg=RegressorWrapper(estimator=LinearRegression(n_jobs=-1))\n","\n","features_pcs,features_eof,features_var = model.eof_features()\n","data_splits = model.forecast_split(features_pcs,model.df.target_ds)\n","# Train on training set,\n","# (need to add crossvalidation),\n","# predict and get error on test set\n","predicted = []\n","\n","for X_train,y_train,X_test,y_test in data_splits:\n","    reg.fit(X_train,y=Target()(y_train))\n","    y_pred = reg.predict(X_test)\n","    predicted.append(y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predicted"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import xarray as xr\n","import numpy as np\n","import pandas as pd\n","\n","# directory \n","\n","datadir = 'Data/'\n","\n","features_fname = datadir+'ERA5.features.1979-2019.JUN-AUG.nc'\n","target_fname = datadir+'precip.mon.total.1x1.v2020.nc'\n","\n","# read in features(CAPE,CIN,GEOPOT,RELHUM)\n","features_ds = xr.open_dataset(features_fname)\n","features.close()\n","\n","# select Amazon range for input fields\n","lat_f_min = -30\n","lat_f_max = 0\n","lon_f_min = -80\n","lon_f_max = -40\n","\n","features = features.sortby(['lat','lon'])\n","features = features.sel(lat=slice(lat_f_min,lat_f_max),lon=slice(lon_f_min,lon_f_max))\n","\n","cape = features.cape\n","cin = features.cin\n","geopot = features.z\n","relhum = features.r\n","time_jja = pd.DatetimeIndex(features.time.data)\n","\n","# read in target(PRECIP)\n","target = xr.open_dataset(target_fname)\n","target.close()\n","\n","# Select Amazon range for target field\n","lat_t_min = -20\n","lat_t_max = -5\n","lon_t_min = -65\n","lon_t_max = -40\n","year_min = 1979\n","\n","target_lon_attrs = target.lon.attrs\n","target['lon'] = xr.where(target['lon']<180,target['lon'],target['lon']-360)\n","target.lon.attrs = target_lon_attrs\n","target = target.sortby(['lon','lat'])\n","target = target.sel(lon=slice(lon_t_min,lon_t_max),lat=slice(lat_t_min,lat_t_max),time=np.logical_and(target['time.year']>=year_min,target['time.season']=='SON'))\n","precip = target['precip']\n","time_son = pd.DatetimeIndex(target.time.data)\n","\n","# features_df = features.to_dataframe(dim_order=['time','lat','lon'])\n","target"]},{"cell_type":"markdown","metadata":{},"source":["Remove linear trends by fitting linear polynomial"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Flatten arrays\n","X = features.to_stacked_array('feature',sample_dims=['time'],name='Features').reset_index('feature')\n","y = target.to_stacked_array('feature',sample_dims=['time'],name='Target').reset_index('feature')\n","\n","# Take seasonal means for JJA and SON\n","# X = X.coarsen(sample=3).mean()\n","print(X.isnull().sum())\n","print(y.isnull().sum())\n","# Remove gridpoints with all nan values\n","X = X.dropna(dim='feature',how='all')\n","y = y.dropna(dim='feature',how='all')\n","print(X.isnull().sum())\n","print(y.isnull().sum())\n","# Interpolate the rest of nan and fill w/ 0.0 for any remaining\n","X = X.fillna()\n","y = y.interpolate_na(dim='time')\n","print(X.isnull().sum())\n","print(y.isnull().sum())\n","X = X.fillna(0)\n","y = y.fillna(0)\n","print(X.isnull().sum())\n","print(y.isnull().sum())\n","# Detrend features\n","X_c = X.polyfit('time',1)\n","X_trends = xr.polyval(X.sample,X_c)\n","X = X- X_trends.to_array().squeeze().drop('variable')\n","\n","# Detrend target\n","y_c = y.polyfit('time',1)\n","y_trends = xr.polyval(y.sample,y_c)\n","y = y- y_trends.to_array().squeeze().drop('variable')\n","X = (X - X.mean(dim='time'))/X.std(dim='time')\n","y = (y - y.mean(dim='time'))/y.std(dim='time')\n","\n","X['time'] = X.time.dt.year\n","y['time'] = y.time.dt.year\n","\n","sample = X.time.values"]},{"cell_type":"markdown","metadata":{},"source":["Split into training and test set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define area weights as sqrt(cos(lat)) for X and y grids\n","X_area_weights = np.sqrt(np.abs(np.cos(np.deg2rad(X.lat.values[np.newaxis,:]))))\n","y_area_weights = np.sqrt(np.abs(np.cos(np.deg2rad(y.lat.values[np.newaxis,:]))))\n","# # Choose test set from 2 dry years and 2 wet years\n","# y_weighted_mean = (y_SON*y_area_weights).mean(dim='feature')\n","# test_yrs = sample[y_weighted_mean.argsort().values]\n","# # Must be >1999 to give >=20 years of training data (minimum bound for PCA)\n","# test_yrs = test_yrs[test_yrs>1989]\n","# test_yrs = np.block([test_yrs[0:2],test_yrs[-2:]])\n","# Choose test years as last 4 yrs in dataset\n","test_yrs = sample[-4:]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import TimeSeriesSplit\n","def time_series_train_test_split(X,y,test_yrs):\n","    # Split features into training and test set\n","    X_train = X.drop_sel(sample=test_yrs)\n","    X_test = X.sel(sample=test_yrs)\n","    # Split target data into dataset for each month\n","    y_sep,y_oct,y_nov = y[y.time.dt.month==9],y[y.time.dt.month==10],y[y.time.dt.month==11]\n","    y_sep_train,y_oct_train,y_nov_train = y_sep.drop_sel(sample=test_yrs),y_oct.drop_sel(sample=test_yrs),y_nov.drop_sel(sample=test_yrs)\n","    y_sep_test,y_oct_test,y_nov_test = y_sep.sel(sample=test_yrs).rename('y_test_September'),y_oct.sel(sample=test_yrs).rename('y_test_October'),y_nov.sel(sample=test_yrs).rename('y_test_November')\n","    return X_train,X_test,y_sep_train,y_oct_train,y_nov_train,y_sep_test,y_oct_test,y_nov_test\n","# Get train,test datasets\n","y_sep,y_oct,y_nov = y[y.time.dt.month==9],y[y.time.dt.month==10],y[y.time.dt.month==11]\n","X_train,X_test,y_sep_train,y_oct_train,y_nov_train,y_sep_test,y_oct_test,y_nov_test = time_series_train_test_split(X,y,test_yrs)\n","\n","# Make cross-validation split for timeseries evaluation\n","cv = TimeSeriesSplit(12,test_size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.base import BaseEstimator,TransformerMixin\n","\n","class weight_collumns(BaseEstimator,TransformerMixin):\n","    def __init__(self,weights):\n","        self.weights=weights\n","    def fit(self,X,y=None):\n","        return self\n","    def transform(self,X):\n","        return X*self.weights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(test_yrs)==np.ndarray"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error,mean_squared_error\n","from sklearn.model_selection import GridSearchCV\n","\n","def predict_score(model,X_train,y_train,X_test,y_test,label,test_yrs=test_yrs):\n","    y_pred_list = []\n","    # Loop through test years'\n","    if type(test_yrs)==np.ndarray:\n","        for yr in test_yrs:\n","            # Fit on leading training data\n","            model.fit(X_train[X_train.sample<yr],y_train[y_train.sample<yr])\n","            # Predict test year\n","            y_pred_res = xr.DataArray(model.predict(X_test.sel(sample=yr).values[np.newaxis,:]).squeeze(),coords=y_test.sel(sample=yr).coords,name=f\"y pred {label}\")\n","            y_pred_list.append(y_pred_res)\n","        # Reformat with xarray\n","        y_pred = xr.concat(y_pred_list,'sample')\n","    elif test_yrs=='all':\n","        model.fit(X_train,y_train)\n","        y_pred = xr.DataArray(model.predict(X_test).squeeze(),coords=y_test.coords,name=f\"y pred {label}\")\n","    mae_test = mean_absolute_error(y_test,y_pred)\n","    rmse_test = mean_squared_error(y_test,y_pred,squared=False)\n","    return y_pred,mae_test,rmse_test\n","\n","def cross_validate(model,X_train,y_train,params,cv):\n","    # initialize gridsearch cross validation\n","    reg_cv = GridSearchCV(model,params,scoring='neg_mean_absolute_error',n_jobs=-1,refit=True,cv=cv)\n","    \n","    # Train model and get results\n","    best_estimator = reg_cv.fit(X_train,y_train).best_estimator_\n","    best_params = reg_cv.best_params_\n","    best_score = -reg_cv.best_score_\n","    cv_results = reg_cv.cv_results_\n","    mae_val = -cv_results['mean_test_score']\n","\n","    return best_estimator,best_params,best_score,cv_results,mae_val\n","\n","\n","def validate(model,X,y,params,cv,test_yrs=test_yrs,return_cv_results=False):\n","    # Get train test split\n","    X_train,X_test,y_sep_train,y_oct_train,y_nov_train,y_sep_test,y_oct_test,y_nov_test = time_series_train_test_split(X,y,test_yrs)\n","    print(\n","        f\"Cross-validation using model: {model['regressor']}\\n\"\n","    )\n","    # Cross validate September model\n","    reg_sep,sep_params,sep_best_score,sep_cv_results,sep_mae_val = cross_validate(model,X_train,y_sep_train,params,cv)\n","    # Cross validate October model\n","    reg_oct,oct_params,oct_best_score,oct_cv_results,oct_mae_val = cross_validate(model,X_train,y_oct_train,params,cv)\n","    # Cross validate November model\n","    reg_nov,nov_params,nov_best_score,nov_cv_results,nov_mae_val = cross_validate(model,X_train,y_nov_train,params,cv)\n","    # Choose best model to use for all three months so they have the same parameters\n","    best_model = [reg_sep,reg_oct,reg_nov][np.argmin([sep_best_score,oct_best_score,nov_best_score])]\n","    best_params = [sep_params,oct_params,nov_params][np.argmin([sep_best_score,oct_best_score,nov_best_score])]\n","    # printout\n","    print(\n","        f\"Minimum Absolute Error of cross-validation:\\n\"\n","        f\"September: {sep_best_score:.3f}, October: {oct_best_score:.3f}, November: {nov_best_score:.3f}\\n\"\n","        f\"Best Hyper-parameters:\\n\"\n","        f\"September: {sep_params}\\nOctober: {oct_params}\\nNovember: {nov_params}\\n\"\n","        f\"Mean Absolute Error of cross-validation:\\n\"\n","        f\"September: {sep_mae_val.mean():.3f} +/- {sep_mae_val.std():.3f}, October: {oct_mae_val.mean():.3f} +/- {oct_mae_val.std():.3f}, November: {nov_mae_val.mean():.3f} +/- {nov_mae_val.std():.3f}\\n\"\n","        f\"Using parameters: {best_params} for prediction\\n\"\n","    )\n","\n","    # Precict September test set\n","    y_sep_pred_test,sep_mae_test,sep_rmse_test = predict_score(best_model,X_train,y_sep_train,X_test,y_sep_test,'test September')\n","    # Precict October test set\n","    y_oct_pred_test,oct_mae_test,oct_rmse_test = predict_score(best_model,X_train,y_oct_train,X_test,y_oct_test,'test Octtember')\n","    # Predict November test set\n","    y_nov_pred_test,nov_mae_test,nov_rmse_test = predict_score(best_model,X_train,y_nov_train,X_test,y_nov_test,'test Novtember')\n","    print(\n","        f\"Absolute Error on test set:\\n\"\n","        f\"September: {sep_mae_test:.3f}, October: {oct_mae_test:.3f}, November: {nov_mae_test:.3f}\\n\"\n","        f\"Root Mean Squared Error on test set:\\n\"\n","        f\"September: {sep_rmse_test:.3f}, October: {oct_rmse_test:.3f}, November: {nov_rmse_test:.3f}\\n\"\n","    )\n","    # Predict on entire dataset\n","    y_sep,y_oct,y_nov=y[y.time.dt.month==9],y[y.time.dt.month==10],y[y.time.dt.month==11]\n","    y_sep_pred,sep_mae,sep_rmse = predict_score(best_model,X,y_sep,X,y_sep,'September',test_yrs='all')\n","    y_oct_pred,oct_mae,oct_rmse = predict_score(best_model,X,y_oct,X,y_oct,'October',test_yrs='all')\n","    y_nov_pred,nov_mae,nov_rmse = predict_score(best_model,X,y_nov,X,y_nov,'November',test_yrs='all')\n","    print(\n","        f\"Absolute Error on entire dataset:\\n\"\n","        f\"September: {sep_mae:.3f}, October: {oct_mae:.3f}, November: {nov_mae:.3f}\\n\"\n","        f\"Root Mean Squared Error on entire dataset:\\n\"\n","        f\"September: {sep_rmse:.3f}, October: {oct_rmse:.3f}, November: {nov_rmse:.3f}\\n\"\n","    )\n","    if return_cv_results==True:\n","        return y_sep_pred_test,y_oct_pred_test,y_nov_pred_test,y_sep_pred,y_oct_pred,y_nov_pred,sep_cv_results,oct_cv_results,nov_cv_results\n","    elif return_cv_results==False:\n","        return y_sep_pred_test,y_oct_pred_test,y_nov_pred_test,y_sep_pred,y_oct_pred,y_nov_pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA,FactorAnalysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","# Make pipeline to do PCA and train model\n","pca_ridge = Pipeline([('scalar',StandardScaler()),('weight',weight_collumns(X_area_weights)),('pca',PCA(svd_solver='full')),('varimax',FactorAnalysis(svd_method='lapack',rotation='varimax')),('regressor',Ridge())])\n","pca_ridge_params = {'pca__n_components':[5,10],'regressor__alpha':[10,1.0,0.1]}\n","y_sep_pred_test_ridge,y_oct_pred_test_ridge,y_nov_pred_test_ridge,y_sep_pred_ridge,y_oct_pred_ridge,y_nov_pred_ridge = validate(pca_ridge,X,y,pca_ridge_params,cv)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import Lasso\n","pca_lasso = Pipeline([('scalar',StandardScaler()),('weight',weight_collumns(X_area_weights)),('pca',PCA(svd_solver='full')),('varimax',FactorAnalysis(svd_method='lapack',rotation='varimax')),('regressor',Lasso())])\n","pca_lasso_params = {'pca__n_components':[5,10],'regressor__alpha':[10,1.0,0.1]}\n","y_sep_pred_test_lasso,y_oct_pred_test_lasso,y_nov_pred_test_lasso,y_sep_pred_lasso,y_oct_pred_lasso,y_nov_pred_lasso = validate(pca_lasso,X,y,pca_lasso_params,cv)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.neural_network import MLPRegressor\n","pca_MLPR = Pipeline([('scalar',StandardScaler()),('weight',weight_collumns(X_area_weights)),('pca',PCA(svd_solver='full')),('varimax',FactorAnalysis(svd_method='lapack',rotation='varimax')),('regressor',MLPRegressor(shuffle=False,max_iter=1000))])\n","pca_MLPR_params = {'pca__n_components':[5,10],'regressor__hidden_layer_sizes':[(5,),(10,),(5,5)],'regressor__alpha':[0.001,0.0001]}\n","y_sep_pred_test_MLPR,y_oct_pred_test_MLPR,y_nov_pred_test_MLPR,y_sep_pred_MLPR,y_oct_pred_MLPR,y_nov_pred_MLPR = validate(pca_MLPR,X,y,pca_MLPR_params,cv)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","def plot_test_pred(y_sep_test,y_oct_test,y_nov_test,y_sep_pred,y_oct_pred,y_nov_pred,test_yrs):\n","    fig,axs = plt.subplots(4,6)\n","    for i,yr in zip(np.arange(len(test_yrs)),test_yrs):\n","        y_sep_test.sel(sample=yr).plot.pcolormesh(x='lon',y='lat',ax=axs[i,0],cmap='rainbow')\n","    for i,yr in zip(np.arange(len(test_yrs)),test_yrs):\n","        y_oct_test.sel(sample=yr).plot.pcolormesh(x='lon',y='lat',ax=axs[i,1],cmap='rainbow')\n","    for i,yr in zip(np.arange(len(test_yrs)),test_yrs):\n","        y_nov_test.sel(sample=yr).plot.pcolormesh(x='lon',y='lat',ax=axs[i,2],cmap='rainbow')\n","    for i,yr in zip(np.arange(len(test_yrs)),test_yrs):\n","        y_sep_pred.sel(sample=yr).plot.pcolormesh(x='lon',y='lat',ax=axs[i,3],cmap='rainbow')\n","    for i,yr in zip(np.arange(len(test_yrs)),test_yrs):\n","        y_oct_pred.sel(sample=yr).plot.pcolormesh(x='lon',y='lat',ax=axs[i,4],cmap='rainbow')\n","    for i,yr in zip(np.arange(len(test_yrs)),test_yrs):\n","        y_nov_pred.sel(sample=yr).plot.pcolormesh(x='lon',y='lat',ax=axs[i,5],cmap='rainbow')\n","    return fig,axs\n","fig,axs = plot_test_pred(y_sep_test,y_oct_test,y_nov_test,y_sep_pred_test_ridge,y_oct_pred_test_ridge,y_nov_pred_test_ridge,test_yrs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["corr_map_sep_ridge = np.corrcoef(y_sep_pred_ridge,y_sep,rowvar=False)[0:375,375:]\n","corr_map_oct_ridge = np.corrcoef(y_oct_pred_ridge,y_oct,rowvar=False)[0:375,375:]\n","corr_map_nov_ridge = np.corrcoef(y_nov_pred_ridge,y_nov,rowvar=False)[0:375,375:]\n","\n","corr_map_sep_lasso = np.corrcoef(y_sep_pred_lasso,y_sep,rowvar=False)[0:375,375:]\n","corr_map_oct_lasso = np.corrcoef(y_oct_pred_lasso,y_oct,rowvar=False)[0:375,375:]\n","corr_map_nov_lasso = np.corrcoef(y_nov_pred_lasso,y_nov,rowvar=False)[0:375,375:]\n","\n","corr_map_sep_MLPR = np.corrcoef(y_sep_pred_MLPR,y_sep,rowvar=False)[0:375,375:]\n","corr_map_oct_MLPR = np.corrcoef(y_oct_pred_MLPR,y_oct,rowvar=False)[0:375,375:]\n","corr_map_nov_MLPR = np.corrcoef(y_nov_pred_MLPR,y_nov,rowvar=False)[0:375,375:]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.pcolormesh(corr_map_sep_lasso)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import holoviews as hv\n","from holoviews.util import opts\n","hv.extension('bokeh')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_true_hv = hv.Dataset(y_test,kdims=['time','sample','month','lat','lon'])\n","y_ridge_hv = hv.Dataset(y_pred_test_ridge,kdims=['time','sample','month','lat','lon'])\n","y_lasso_hv = hv.Dataset(y_pred_test_lasso,kdims=['time','sample','month','lat','lon'])\n","y_MLPR_hv = hv.Dataset(y_pred_test_MLPR,kdims=['time','sample','month','lat','lon'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_true_hv.to(hv.Image,kdims=['lat','lon'],groupby=['sample','month'],dynamic=False).layout()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n","def plot_error(y_sep_test,y_oct_test,y_nov_test,y_sep_pred,y_oct_pred,y_nov_pred):\n","    fig,axs = plt.subplots(1,3,figsize=(24,8))\n","    axs[0].scatter(y_sep_test,y_sep_pred)\n","    axs[0].plot(np.array([-3,3]),np.array([-3,3]),'k--')\n","    axs[0].set_xlabel('y true')\n","    axs[0].set_ylabel('y pred')\n","    axs[0].set_xlim(-3,3)\n","    axs[0].set_ylim(-3,3)\n","    axs[0].set_title('September')\n","\n","    axs[1].scatter(y_oct_test,y_oct_pred)\n","    axs[1].plot(np.array([-3,3]),np.array([-3,3]),'k--')\n","    axs[1].set_xlabel('y true')\n","    axs[1].set_ylabel('y pred')\n","    axs[1].set_xlim(-3,3)\n","    axs[1].set_ylim(-3,3)\n","    axs[1].set_title('October')\n","\n","    axs[2].scatter(y_nov_test,y_nov_pred)\n","    axs[2].plot(np.array([-3,3]),np.array([-3,3]),'k--')\n","    axs[2].set_xlabel('y true')\n","    axs[2].set_ylabel('y pred')\n","    axs[2].set_xlim(-3,3)\n","    axs[2].set_ylim(-3,3)\n","    axs[2].set_title('November')\n","    fig.tight_layout()\n","    return fig,axs\n","fig,axs = plot_error(y_sep_test,y_oct_test,y_nov_test,y_sep_pred_test_ridge,y_oct_pred_test_ridge,y_nov_pred_test_ridge)\n","fig,axs = plot_error(y_sep_test,y_oct_test,y_nov_test,y_sep_pred_test_lasso,y_oct_pred_test_lasso,y_nov_pred_test_lasso)\n","fig,axs = plot_error(y_sep_test,y_oct_test,y_nov_test,y_sep_pred_test_MLPR,y_oct_pred_test_MLPR,y_nov_pred_test_MLPR)"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import matplotlib as mpl\n","\n","%matplotlib inline\n","\n","fig,axs = plt.subplots(5,3,figsize=(20,20))\n","cape.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[0,0],x='time',label=cape.name)\n","cin.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[1,0],x='time',label=cin.name)\n","geopot.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[2,0],x='time',label=geopot.name)\n","relhum.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[3,0],x='time',label=relhum.name)\n","precip.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[4,0],x='time',label=precip.name)\n","\n","cape_trend.mean(dim=['lat','lon'],keep_attrs=True).plot.line('k--',ax=axs[0,0],x='time',label='trend')\n","cin_trend.mean(dim=['lat','lon'],keep_attrs=True).plot.line('k--',ax=axs[1,0],x='time',label='trend')\n","geopot_trend.mean(dim=['lat','lon'],keep_attrs=True).plot.line('k--',ax=axs[2,0],x='time',label='trend')\n","relhum_trend.mean(dim=['lat','lon'],keep_attrs=True).plot.line('k--',ax=axs[3,0],x='time',label='trend')\n","precip_trend.mean(dim=['lat','lon'],keep_attrs=True).plot.line('k--',ax=axs[4,0],x='time',label='trend')\n","\n","axs[0,0].set_ylabel(cape.long_name+' ['+cape.units+']')\n","axs[1,0].set_ylabel(cin.long_name+' ['+cin.units+']')\n","axs[2,0].set_ylabel(geopot.long_name+' ['+geopot.units+']')\n","axs[3,0].set_ylabel(relhum.long_name+' ['+relhum.units+']')\n","axs[4,0].set_ylabel(precip.long_name+' ['+precip.units+']')\n","\n","cape_anm.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[0,1],x='time',label=cape_anm.name)\n","cin_anm.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[1,1],x='time',label=cin_anm.name)\n","geopot_anm.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[2,1],x='time',label=geopot_anm.name)\n","relhum_anm.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[3,1],x='time',label=relhum_anm.name)\n","precip_anm.mean(dim=['lat','lon'],keep_attrs=True).plot.line(ax=axs[4,1],x='time',label=precip_anm.name)\n","\n","cape.mean(dim='time',keep_attrs=True).plot(ax=axs[0,2],cmap='Spectral')\n","cin.mean(dim='time',keep_attrs=True).plot(ax=axs[1,2],cmap='Spectral')\n","geopot.mean(dim='time',keep_attrs=True).plot(ax=axs[2,2],cmap='Spectral')\n","relhum.mean(dim='time',keep_attrs=True).plot(ax=axs[3,2],cmap='Spectral')\n","precip.mean(dim='time',keep_attrs=True).plot(ax=axs[4,2],cmap='Spectral')\n","\n","# axs[0,2].set_title(cape.long_name+' climatology')\n","# axs[1,2].set_title(cin.long_name+' climatology')\n","# axs[2,2].set_title(geopot.long_name+' climatology')\n","# axs[3,2].set_title(relhum.long_name+' climatology')\n","# axs[4,2].set_title(precip.long_name+' climatology')\n","\n","for ax in axs[0:4,0:2].flat:\n","    ax.grid()\n","    ax.set_xlim(time_jja.min(),time_jja.max())\n","    ax.legend()\n","for ax in axs[4,0:2].flat:\n","    ax.grid()\n","    ax.set_xlim(time_son.min(),time_son.max())\n","    ax.legend()\n","for ax in axs[:,2].flat:\n","    ax.grid()\n","\n","fig.tight_layout()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Multivariate EOF\n","- Use eofs library to calculate MEOF on the 4 input fields\n","- Dimmensionality reduction: retain first 10 PCs to be used as inputs for models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from eofs.multivariate.standard import MultivariateEof\n","# Weights arrays\n","weights_array = np.sqrt(np.abs(np.cos(np.deg2rad(features.lat.data))))[:, np.newaxis]\n","weights_arrayp = np.sqrt(np.abs(np.cos(np.deg2rad(target.lat.data))))[:, np.newaxis]\n","ncomp = 10\n","# MEOF on 4 input variables.\n","solver = MultivariateEof([cape_anm_JJA.data,cin_anm_JJA.data,geopot_anm_JJA.data,relhum_anm_JJA.data],weights=[weights_array,weights_array,weights_array,weights_array])\n","meof10_list = solver.eofs(neofs=ncomp,eofscaling=2)\n","meof10_pcs  = solver.pcs(npcs=ncomp,pcscaling=0)\n","meof10_pcs = meof10_pcs / meof10_pcs.std()\n","meof10_vfrc = solver.varianceFraction(neigs=ncomp)\n","\n","cape_meof10 = meof10_list[0]\n","cin_meof10 = meof10_list[1]\n","geopot_meof10 = meof10_list[2]\n","relhum_meof10 = meof10_list[3]\n","\n","print(\"First 10 pcs account for %0.3f %% of the variance\" % (np.sum(meof10_vfrc)*100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# sm = mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(-1,1),cmap='Spectral')\n","# fig,axs = ortho_subplots(1,3)\n","# fig.suptitle('MEOF Structure for CAPE')\n","# fill = axs[0].contourf(lon_f,lat_f,cape_meof10[0],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[0].set_title('MEOF 1')\n","# fill = axs[1].contourf(lon_f,lat_f,cape_meof10[1],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[1].set_title('MEOF 2')\n","# fill = axs[2].contourf(lon_f,lat_f,cape_meof10[2],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[2].set_title('MEOF 3')\n","# fig.tight_layout()\n","# fig.colorbar(sm,ax=axs[:],location='bottom',shrink=.6)\n","\n","# fig,axs = ortho_subplots(1,3)\n","# fig.suptitle('MEOF Structure for CIN')\n","# fill = axs[0].contourf(lon_f,lat_f,cin_meof10[0],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[0].set_title('MEOF 1')\n","# fill = axs[1].contourf(lon_f,lat_f,cin_meof10[1],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[1].set_title('MEOF 2')\n","# fill = axs[2].contourf(lon_f,lat_f,cin_meof10[2],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[2].set_title('MEOF 3')\n","# fig.tight_layout()\n","# fig.colorbar(sm,ax=axs[:],location='bottom',shrink=.6)\n","\n","# fig,axs = ortho_subplots(1,3)\n","# fig.suptitle('MEOF Structure for Geopotential')\n","# fill = axs[0].contourf(lon_f,lat_f,geopot_meof10[0],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[0].set_title('MEOF 1')\n","# fill = axs[1].contourf(lon_f,lat_f,geopot_meof10[1],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[1].set_title('MEOF 2')\n","# fill = axs[2].contourf(lon_f,lat_f,geopot_meof10[2],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[2].set_title('MEOF 3')\n","# fig.tight_layout()\n","# fig.colorbar(sm,ax=axs[:],location='bottom',shrink=.6)\n","\n","# fig,axs = ortho_subplots(1,3)\n","# fig.suptitle('MEOF Structure for Relative Humidity')\n","# fill = axs[0].contourf(lon_f,lat_f,relhum_meof10[0],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[0].set_title('MEOF 1')\n","# fill = axs[1].contourf(lon_f,lat_f,relhum_meof10[1],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[1].set_title('MEOF 2')\n","# fill = axs[2].contourf(lon_f,lat_f,relhum_meof10[2],transform=ccrs.PlateCarree(),levels=100,vmin=-1,vmax=1,cmap='Spectral')\n","# axs[2].set_title('MEOF 3')\n","# fig.tight_layout()\n","# fig.colorbar(sm,ax=axs[:],location='bottom',shrink=.6)\n","\n","\n","fig,ax = plt.subplots(figsize=(16,9),dpi=120)\n","ax.plot(cape_anm_JJA.time.data,meof10_pcs[:,0],label='PC 1')\n","ax.plot(cape_anm_JJA.time.data,meof10_pcs[:,1],label='PC 2')\n","ax.plot(cape_anm_JJA.time.data,meof10_pcs[:,2],label='PC 3')\n","ax.legend()\n","ax.set_title('PC Timeseries for MEOF')\n","ax.set_xlabel('year')\n","ax.set_xlim(cape_anm_JJA.time.data.min(),cape_anm_JJA.time.data.max())\n","ax.axhline(c='black',linestyle='--')\n","fig.tight_layout()\n","\n","fig,axs = plt.subplots(1,2,figsize=(16,9),dpi=120)\n","fig.suptitle('Explained Variance of MEOF PC Modes')\n","axs[0].plot(np.arange(len(meof10_vfrc))+1,meof10_vfrc,'-o')\n","axs[0].set_title('Fraction of Variance')\n","axs[0].set_xlim(1,10)\n","axs[0].set_xlabel('PC Mode')\n","axs[0].set_ylim(0)\n","axs[1].plot(np.arange(len(meof10_vfrc))+1,np.cumsum(meof10_vfrc),'-o')\n","axs[1].set_title('Cumulative Variance')\n","axs[1].set_xlim(1,10)\n","axs[1].set_xlabel('PC Mode')\n","axs[1].set_ylim(0,1);\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["## Train Model\n","- Use TimeSeriesSplit for k-fold cross-validation to\n","- Train 3 models September, October, November"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn_xarray import wrap, Target\n","from sklearn.model_selection import TimeSeriesSplit,GridSearchCV\n","from sklearn.linear_model import Ridge,Lasso\n","from sklearn.neural_network import MLPRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split precipitation into sep,oct,nov for 3 models\n","precip_sep = precip_anm\n","precip_oct = precip_anm\n","precip_nov = precip_anm\n","\n","n = len(years)\n","n_test = 4\n","n_train = n-n_test\n","frac = n_test/n_train\n","print('(# training samples, # test samples, fraction) = (%i, %i, %0.3f)'%(n_train,n_test,frac))\n","\n","# test_idx = years == 2010\n","test_idx = years >= years[n-n_test]\n","train_idx = np.logical_not(test_idx)\n","\n","years_train = years[train_idx]\n","years_test = years[test_idx]\n","\n","X_train = meof10_pcs[train_idx]\n","X_test = meof10_pcs[test_idx]\n","\n","# Save precip shape so we can make it 2D for sklearn\n","p_shape = precip_sep.shape\n","\n","y_train_sep = precip_sep[train_idx].reshape(n_train,-1)\n","y_test_sep = precip_sep[test_idx].reshape(n_test,-1)\n","\n","y_train_oct = precip_oct[train_idx].reshape(n_train,-1)\n","y_test_oct = precip_oct[test_idx].reshape(n_test,-1)\n","\n","y_train_nov = precip_nov[train_idx].reshape(n_train,-1)\n","y_test_nov = precip_nov[test_idx].reshape(n_test,-1)\n","\n","y_test_sep_3d = y_test_sep.reshape(n_test,p_shape[1],p_shape[2])\n","y_test_oct_3d = y_test_oct.reshape(n_test,p_shape[1],p_shape[2])\n","y_test_nov_3d = y_test_nov.reshape(n_test,p_shape[1],p_shape[2])\n","\n","print('X_train shape =', X_train.shape)\n","print('y_train shape =', y_train_nov.shape)\n","print('X_test shape =', X_test.shape)\n","print('y_test shape =', y_test_nov.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# REC curve\n","tol_max = 2.5\n","delta = 10*np.finfo(float).eps\n","tolerance = np.arange(0,tol_max+delta,0.05)\n","\n","def rec(m,n,tol):\n","    if type(m)!='numpy.ndarray':\n","        m=np.array(m)\n","    if type(n)!='numpy.ndarray':\n","        n=np.array(n)\n","    l=m.size\n","    accuracy = 0\n","    for i in range(l):\n","        if np.abs(m[i]-n[i])<=tol:\n","            accuracy+=1\n","    return (accuracy/l)"]},{"cell_type":"markdown","metadata":{},"source":["#### Ridge"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import Ridge\n","\n","k = 12\n","cvsplit = TimeSeriesSplit(n_splits=k,test_size=3)\n","cvsplit.split(X_train)\n","\n","parameters = {'alpha':[0.001,0.01,0.1,1,10,20]}\n","\n","clf = GridSearchCV(Ridge(),parameters,scoring='neg_mean_squared_error',n_jobs=-1,cv=cvsplit)\n","clf.fit(X_train,y_train_sep)\n","\n","print(\"Best parameters set found on September development set:\",clf.best_params_)\n","print(\"MSE on test set: %0.3f\" % -clf.score(X_test,y_test_sep))\n","\n","clf_sep = clf.best_estimator_\n","\n","# -------------------\n","\n","clf.fit(X_train,y_train_oct)\n","\n","print(\"\\nBest parameters set found on October development set:\",clf.best_params_)\n","print(\"MSE on test set: %0.3f\" % -clf.score(X_test,y_test_oct))\n","\n","clf_oct = clf.best_estimator_\n","\n","# ----------------------\n","\n","clf.fit(X_train,y_train_nov)\n","\n","print(\"\\nBest parameters set found on November development set:\",clf.best_params_)\n","print(\"MSE on test set: %0.3f\" % -clf.score(X_test,y_test_nov))\n","\n","clf_nov = clf.best_estimator_\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred_sep_Ridge = clf_sep.predict(meof10_pcs).reshape(p_shape)\n","y_pred_oct_Ridge = clf_oct.predict(meof10_pcs).reshape(p_shape)\n","y_pred_nov_Ridge = clf_nov.predict(meof10_pcs).reshape(p_shape)\n","\n","y_test_pred_sep_Ridge = clf_sep.predict(X_test).reshape(-1,p_shape[1],p_shape[2])\n","y_test_pred_oct_Ridge = clf_oct.predict(X_test).reshape(-1,p_shape[1],p_shape[2])\n","y_test_pred_nov_Ridge = clf_nov.predict(X_test).reshape(-1,p_shape[1],p_shape[2])\n","\n","rec_Ridge_sep = np.empty(len(tolerance))\n","rec_Ridge_oct = np.empty(len(tolerance))\n","rec_Ridge_nov = np.empty(len(tolerance))\n","for t,i in zip(tolerance,range(len(tolerance))):\n","    rec_Ridge_sep[i] = rec(y_test_pred_sep_Ridge.ravel(),y_test_sep.ravel(),t)\n","    rec_Ridge_oct[i] = rec(y_test_pred_oct_Ridge.ravel(),y_test_oct.ravel(),t)\n","    rec_Ridge_nov[i] = rec(y_test_pred_nov_Ridge.ravel(),y_test_nov.ravel(),t)\n","\n","corr_sep_Ridge = np.empty(p_shape[1:])\n","for lat in np.arange(len(lat_t)):\n","    for lon in np.arange(len(lon_t)):\n","        corr_sep_Ridge[lat,lon] = np.corrcoef(y_pred_sep_Ridge[:,lat,lon],precip_sep[:,lat,lon])[0,1]\n","corr_oct_Ridge = np.empty(p_shape[1:])\n","for lat in np.arange(len(lat_t)):\n","    for lon in np.arange(len(lon_t)):\n","        corr_oct_Ridge[lat,lon] = np.corrcoef(y_pred_oct_Ridge[:,lat,lon],precip_oct[:,lat,lon])[0,1]\n","corr_nov_Ridge = np.empty(p_shape[1:])\n","for lat in np.arange(len(lat_t)):\n","    for lon in np.arange(len(lon_t)):\n","        corr_nov_Ridge[lat,lon] = np.corrcoef(y_pred_nov_Ridge[:,lat,lon],precip_nov[:,lat,lon])[0,1]\n","\n","if n_test == 1:\n","    fig,axs = plt.subplots(n_test,6,figsize=(16,n_test*3),dpi=120)\n","    axs[0].contourf(lon_t,lat_t,y_test_sep_3d[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[0].set_title('Sep')\n","    axs[0].set_ylabel(years_test[0])\n","\n","    axs[1].contourf(lon_t,lat_t,y_test_oct_3d[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[1].set_title('Oct')\n","\n","    axs[2].contourf(lon_t,lat_t,y_test_nov_3d[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[2].set_title('Nov')\n","\n","    axs[3].contourf(lon_t,lat_t,y_test_pred_sep_Ridge[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[3].set_title('Sep Prediction')\n","\n","    axs[4].contourf(lon_t,lat_t,y_test_pred_oct_Ridge[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[4].set_title('Oct Prediction')\n","\n","    axs[5].contourf(lon_t,lat_t,y_test_pred_nov_Ridge[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[5].set_title('Nov Prediction')\n","    plt.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(vmin,vmax),cmap='Spectral'),ax=axs[2:4],location='bottom')\n","    plt.tight_layout(rect=[0.1,.2,.9,.9])\n","\n","else:\n","    fig,axs = plt.subplots(n_test,6,figsize=(16,n_test*3),dpi=120)\n","    for i,ax in zip(np.arange(n_test),axs.T[0]):\n","        ax.contourf(lon_t,lat_t,y_test_sep_3d[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Sep')\n","        ax.set_ylabel(years_test[i])\n","    for i,ax in zip(np.arange(n_test),axs[:,1]):\n","        ax.contourf(lon_t,lat_t,y_test_oct_3d[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Oct')\n","    for i,ax in zip(np.arange(n_test),axs[:,2]):\n","        ax.contourf(lon_t,lat_t,y_test_nov_3d[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Nov')\n","    for i,ax in zip(np.arange(n_test),axs[:,3]):\n","        ax.contourf(lon_t,lat_t,y_test_pred_sep_Ridge[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Sep Prediction')\n","    for i,ax in zip(np.arange(n_test),axs[:,4]):\n","        ax.contourf(lon_t,lat_t,y_test_pred_oct_Ridge[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Oct Prediction')\n","    for i,ax in zip(np.arange(n_test),axs[:,5]):\n","        ax.contourf(lon_t,lat_t,y_test_pred_nov_Ridge[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Nov Prediction')\n","    plt.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(vmin,vmax),cmap='Spectral'),ax=axs[-1,2:4],location='bottom')\n","    plt.tight_layout(rect=[0.1,.2,.9,.9])\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Lasso"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import Lasso\n","parameters = {'alpha':[0.0001,0.001,0.01,0.05],}\n","clf = GridSearchCV(Lasso(),parameters,scoring='neg_mean_squared_error',n_jobs=-1,cv=cvsplit)\n","clf.fit(X_train,y_train_sep)\n","\n","print(\"Best parameters set found on September development set:\",clf.best_params_)\n","print(\"MSE on test set: %0.3f\" % -clf.score(X_test,y_test_sep))\n","\n","clf_sep = clf.best_estimator_\n","\n","# --------------------------\n","\n","clf.fit(X_train,y_train_oct)\n","\n","print(\"\\nBest parameters set found on October development set:\",clf.best_params_)\n","print(\"MSE on test set: %0.3f\" % -clf.score(X_test,y_test_oct))\n","\n","clf_oct = clf.best_estimator_\n","\n","# ---------------------------\n","\n","clf.fit(X_train,y_train_nov)\n","\n","print(\"\\nBest parameters set found on November development set:\",clf.best_params_)\n","print(\"MSE on test set: %0.3f\" % -clf.score(X_test,y_test_nov))\n","\n","clf_nov = clf.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred_sep_Lasso = clf_sep.predict(meof10_pcs).reshape(p_shape)\n","y_pred_oct_Lasso = clf_oct.predict(meof10_pcs).reshape(p_shape)\n","y_pred_nov_Lasso = clf_nov.predict(meof10_pcs).reshape(p_shape)\n","\n","y_test_pred_sep_Lasso = clf_sep.predict(X_test).reshape(-1,p_shape[1],p_shape[2])\n","y_test_pred_oct_Lasso = clf_oct.predict(X_test).reshape(-1,p_shape[1],p_shape[2])\n","y_test_pred_nov_Lasso = clf_nov.predict(X_test).reshape(-1,p_shape[1],p_shape[2])\n","\n","rec_Lasso_sep = np.empty(len(tolerance))\n","rec_Lasso_oct = np.empty(len(tolerance))\n","rec_Lasso_nov = np.empty(len(tolerance))\n","for t,i in zip(tolerance,range(len(tolerance))):\n","    rec_Lasso_sep[i] = rec(y_test_pred_sep_Lasso.ravel(),y_test_sep.ravel(),t)\n","    rec_Lasso_oct[i] = rec(y_test_pred_oct_Lasso.ravel(),y_test_oct.ravel(),t)\n","    rec_Lasso_nov[i] = rec(y_test_pred_nov_Lasso.ravel(),y_test_nov.ravel(),t)\n","\n","corr_sep_Lasso = np.empty(p_shape[1:])\n","for lat in np.arange(len(lat_t)):\n","    for lon in np.arange(len(lon_t)):\n","        corr_sep_Lasso[lat,lon] = np.corrcoef(y_pred_sep_Lasso[:,lat,lon],precip_sep[:,lat,lon])[0,1]\n","corr_oct_Lasso = np.empty(p_shape[1:])\n","for lat in np.arange(len(lat_t)):\n","    for lon in np.arange(len(lon_t)):\n","        corr_oct_Lasso[lat,lon] = np.corrcoef(y_pred_oct_Lasso[:,lat,lon],precip_oct[:,lat,lon])[0,1]\n","corr_nov_Lasso = np.empty(p_shape[1:])\n","for lat in np.arange(len(lat_t)):\n","    for lon in np.arange(len(lon_t)):\n","        corr_nov_Lasso[lat,lon] = np.corrcoef(y_pred_nov_Lasso[:,lat,lon],precip_nov[:,lat,lon])[0,1]\n","\n","if n_test == 1:\n","    fig,axs = plt.subplots(n_test,6,figsize=(16,n_test*3),dpi=120)\n","    axs[0].contourf(lon_t,lat_t,y_test_sep_3d[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[0].set_title('Sep')\n","    axs[0].set_ylabel(years_test[0])\n","\n","    axs[1].contourf(lon_t,lat_t,y_test_oct_3d[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[1].set_title('Oct')\n","\n","    axs[2].contourf(lon_t,lat_t,y_test_nov_3d[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[2].set_title('Nov')\n","\n","    axs[3].contourf(lon_t,lat_t,y_test_pred_sep_Lasso[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[3].set_title('Sep Prediction')\n","\n","    axs[4].contourf(lon_t,lat_t,y_test_pred_oct_Lasso[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[4].set_title('Oct Prediction')\n","\n","    axs[5].contourf(lon_t,lat_t,y_test_pred_nov_Lasso[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[5].set_title('Nov Prediction')\n","    plt.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(vmin,vmax),cmap='Spectral'),ax=axs[2:4],location='bottom')\n","    plt.tight_layout(rect=[0.1,.2,.9,.9])\n","\n","else:\n","    fig,axs = plt.subplots(n_test,6,figsize=(16,n_test*3),dpi=120)\n","    for i,ax in zip(np.arange(n_test),axs.T[0]):\n","        ax.contourf(lon_t,lat_t,y_test_sep_3d[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Sep')\n","        ax.set_ylabel(years_test[i])\n","    for i,ax in zip(np.arange(n_test),axs[:,1]):\n","        ax.contourf(lon_t,lat_t,y_test_oct_3d[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Oct')\n","    for i,ax in zip(np.arange(n_test),axs[:,2]):\n","        ax.contourf(lon_t,lat_t,y_test_nov_3d[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Nov')\n","    for i,ax in zip(np.arange(n_test),axs[:,3]):\n","        ax.contourf(lon_t,lat_t,y_test_pred_sep_Lasso[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Sep Prediction')\n","    for i,ax in zip(np.arange(n_test),axs[:,4]):\n","        ax.contourf(lon_t,lat_t,y_test_pred_oct_Lasso[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Oct Prediction')\n","    for i,ax in zip(np.arange(n_test),axs[:,5]):\n","        ax.contourf(lon_t,lat_t,y_test_pred_nov_Lasso[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Nov Prediction')\n","    plt.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(vmin,vmax),cmap='Spectral'),ax=axs[-1,2:4],location='bottom')\n","    plt.tight_layout(rect=[0.1,.2,.9,.9])"]},{"cell_type":"markdown","metadata":{},"source":["## Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.neural_network import MLPRegressor\n","parameters = {'hidden_layer_sizes':[5],'alpha':[0.0001,0.001],'activation':['relu']}\n","\n","clf = GridSearchCV(MLPRegressor(shuffle=False,max_iter=1000),parameters,scoring='neg_mean_squared_error',n_jobs=-1,cv=cvsplit)\n","clf.fit(X_train,y_train_sep)\n","\n","print(\"Best parameters set found on September development set:\",clf.best_params_)\n","print(\"MSE on test set: %0.3f\" % -clf.score(X_test,y_test_sep))\n","\n","clf_sep = clf.best_estimator_\n","\n","# --------------------------\n","\n","clf.fit(X_train,y_train_oct)\n","\n","print(\"\\nBest parameters set found on October development set:\",clf.best_params_)\n","print(\"MSE on test set: %0.3f\" % -clf.score(X_test,y_test_oct))\n","\n","clf_oct = clf.best_estimator_\n","\n","# ---------------------------\n","\n","clf.fit(X_train,y_train_nov)\n","\n","print(\"\\nBest parameters set found on November development set:\",clf.best_params_)\n","print(\"MSE on test set: %0.3f\" % -clf.score(X_test,y_test_nov))\n","\n","clf_nov = clf.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred_sep_MLPR = clf_sep.predict(meof10_pcs).reshape(p_shape)\n","y_pred_oct_MLPR = clf_oct.predict(meof10_pcs).reshape(p_shape)\n","y_pred_nov_MLPR = clf_nov.predict(meof10_pcs).reshape(p_shape)\n","\n","y_test_pred_sep_MLPR = clf_sep.predict(X_test).reshape(-1,p_shape[1],p_shape[2])\n","y_test_pred_oct_MLPR = clf_oct.predict(X_test).reshape(-1,p_shape[1],p_shape[2])\n","y_test_pred_nov_MLPR = clf_nov.predict(X_test).reshape(-1,p_shape[1],p_shape[2])\n","\n","rec_MLPR_sep = np.empty(len(tolerance))\n","rec_MLPR_oct = np.empty(len(tolerance))\n","rec_MLPR_nov = np.empty(len(tolerance))\n","for t,i in zip(tolerance,range(len(tolerance))):\n","    rec_MLPR_sep[i] = rec(y_test_pred_sep_MLPR.ravel(),y_test_sep.ravel(),t)\n","    rec_MLPR_oct[i] = rec(y_test_pred_oct_MLPR.ravel(),y_test_oct.ravel(),t)\n","    rec_MLPR_nov[i] = rec(y_test_pred_nov_MLPR.ravel(),y_test_nov.ravel(),t)\n","\n","corr_sep_MLPR = np.empty(p_shape[1:])\n","for lat in np.arange(len(lat_t)):\n","    for lon in np.arange(len(lon_t)):\n","        corr_sep_MLPR[lat,lon] = np.corrcoef(y_pred_sep_MLPR[:,lat,lon],precip_sep[:,lat,lon])[0,1]\n","corr_oct_MLPR = np.empty(p_shape[1:])\n","for lat in np.arange(len(lat_t)):\n","    for lon in np.arange(len(lon_t)):\n","        corr_oct_MLPR[lat,lon] = np.corrcoef(y_pred_oct_MLPR[:,lat,lon],precip_oct[:,lat,lon])[0,1]\n","corr_nov_MLPR = np.empty(p_shape[1:])\n","for lat in np.arange(len(lat_t)):\n","    for lon in np.arange(len(lon_t)):\n","        corr_nov_MLPR[lat,lon] = np.corrcoef(y_pred_nov_MLPR[:,lat,lon],precip_nov[:,lat,lon])[0,1]\n","\n","if n_test == 1:\n","    fig,axs = plt.subplots(n_test,6,figsize=(16,n_test*3),dpi=120)\n","    axs[0].contourf(lon_t,lat_t,y_test_sep_3d[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[0].set_title('Sep')\n","    axs[0].set_ylabel(years_test[0])\n","\n","    axs[1].contourf(lon_t,lat_t,y_test_oct_3d[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[1].set_title('Oct')\n","\n","    axs[2].contourf(lon_t,lat_t,y_test_nov_3d[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[2].set_title('Nov')\n","\n","    axs[3].contourf(lon_t,lat_t,y_test_pred_sep_MLPR[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[3].set_title('Sep Prediction')\n","\n","    axs[4].contourf(lon_t,lat_t,y_test_pred_oct_MLPR[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[4].set_title('Oct Prediction')\n","\n","    axs[5].contourf(lon_t,lat_t,y_test_pred_nov_MLPR[0],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","    axs[5].set_title('Nov Prediction')\n","    plt.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(vmin,vmax),cmap='Spectral'),ax=axs[2:4],location='bottom')\n","    plt.tight_layout(rect=[0.1,.2,.9,.9])\n","\n","else:\n","    fig,axs = plt.subplots(n_test,6,figsize=(16,n_test*3),dpi=120)\n","    for i,ax in zip(np.arange(n_test),axs.T[0]):\n","        ax.contourf(lon_t,lat_t,y_test_sep_3d[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Sep')\n","        ax.set_ylabel(years_test[i])\n","    for i,ax in zip(np.arange(n_test),axs[:,1]):\n","        ax.contourf(lon_t,lat_t,y_test_oct_3d[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Oct')\n","    for i,ax in zip(np.arange(n_test),axs[:,2]):\n","        ax.contourf(lon_t,lat_t,y_test_nov_3d[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Nov')\n","    for i,ax in zip(np.arange(n_test),axs[:,3]):\n","        ax.contourf(lon_t,lat_t,y_test_pred_sep_MLPR[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Sep Prediction')\n","    for i,ax in zip(np.arange(n_test),axs[:,4]):\n","        ax.contourf(lon_t,lat_t,y_test_pred_oct_MLPR[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Oct Prediction')\n","    for i,ax in zip(np.arange(n_test),axs[:,5]):\n","        ax.contourf(lon_t,lat_t,y_test_pred_nov_MLPR[i],levels=100,vmin=vmin,vmax=vmax,cmap='Spectral')\n","        ax.set_title('Nov Prediction')\n","    plt.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(vmin,vmax),cmap='Spectral'),ax=axs[-1,2:4],location='bottom')\n","    plt.tight_layout(rect=[0.1,.2,.9,.9])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig,axs = plt.subplots(1,3,figsize=(12,4),dpi=120)\n","axs[0].set_title(\"Sep\\n\")\n","axs[1].set_title(\"Oct\\n\")\n","axs[2].set_title(\"Nov\\n\")\n","axs[0].plot(tolerance,rec_Ridge_sep,label=\"Ridge\")\n","axs[0].plot(tolerance,rec_Lasso_sep,label=\"Lasso\")\n","axs[0].plot(tolerance,rec_MLPR_sep,label=\"NN\")\n","axs[1].plot(tolerance,rec_Ridge_oct,label=\"Ridge\")\n","axs[1].plot(tolerance,rec_Lasso_oct,label=\"Lasso\")\n","axs[1].plot(tolerance,rec_MLPR_oct,label=\"NN\")\n","axs[2].plot(tolerance,rec_Ridge_nov,label=\"Ridge\")\n","axs[2].plot(tolerance,rec_Lasso_nov,label=\"Lasso\")\n","axs[2].plot(tolerance,rec_MLPR_nov,label=\"NN\")\n","axs[0].set_xlabel(\"Absolute deviation\")\n","axs[0].set_ylabel(\"Accuracy\")\n","axs[1].set_xlabel(\"Absolute deviation\")\n","axs[1].set_ylabel(\"Accuracy\")\n","axs[2].set_xlabel(\"Absolute deviation\")\n","axs[2].set_ylabel(\"Accuracy\")\n","axs[0].grid()\n","axs[1].grid()\n","axs[2].grid()\n","axs[0].legend()\n","axs[1].legend()\n","axs[2].legend()\n","fig.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig,axs = plt.subplots(3,3,figsize=(12,6),dpi=120)\n","axs[0,0].contourf(lon_t,lat_t,corr_sep_Ridge,levels=100,cmap='seismic',vmin=-1,vmax=1)\n","axs[1,0].contourf(lon_t,lat_t,corr_oct_Ridge,levels=100,cmap='seismic',vmin=-1,vmax=1)\n","axs[2,0].contourf(lon_t,lat_t,corr_nov_Ridge,levels=100,cmap='seismic',vmin=-1,vmax=1)\n","axs[0,0].set_title('Ridge')\n","axs[0,0].set_ylabel('Sep')\n","axs[1,0].set_ylabel('Oct')\n","axs[2,0].set_ylabel('Nov')\n","\n","axs[0,1].contourf(lon_t,lat_t,corr_sep_Lasso,levels=100,cmap='seismic',vmin=-1,vmax=1)\n","axs[1,1].contourf(lon_t,lat_t,corr_oct_Lasso,levels=100,cmap='seismic',vmin=-1,vmax=1)\n","axs[2,1].contourf(lon_t,lat_t,corr_nov_Lasso,levels=100,cmap='seismic',vmin=-1,vmax=1)\n","axs[0,1].set_title('Lasso')\n","\n","axs[0,2].contourf(lon_t,lat_t,corr_sep_MLPR,levels=100,cmap='seismic',vmin=-1,vmax=1)\n","axs[1,2].contourf(lon_t,lat_t,corr_oct_MLPR,levels=100,cmap='seismic',vmin=-1,vmax=1)\n","axs[2,2].contourf(lon_t,lat_t,corr_nov_MLPR,levels=100,cmap='seismic',vmin=-1,vmax=1)\n","axs[0,2].set_title('NN Regressor')\n","\n","plt.colorbar(mpl.cm.ScalarMappable(norm=mpl.colors.Normalize(-1,1),cmap='seismic'),ax=axs[:],shrink=0.6)\n","fig.tight_layout(rect=[0.1,.1,.75,.9])"]}],"metadata":{"interpreter":{"hash":"0f595d3fc34ed11edbc38d9dec2a54c03fa2bf2c415ca6a655d899d5167c3720"},"kernelspec":{"display_name":"Python 3.9.6 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
